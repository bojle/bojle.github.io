<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Optimizing ONNX QuantizeLinear with ARM NEON for Efficient Edge Inference &#8212; Thoughts, et cetera  documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=686e5160" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=a966dca0" />
    <script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="icon" href="../_static/flying-katakana-man.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Why Even Bother With FPGAs?" href="why_even_bother_with_fpgas.html" />
    <link rel="prev" title="Blog" href="index.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="optimizing-onnx-quantizelinear-with-arm-neon-for-efficient-edge-inference">
<span id="onnx-quantizelinear-neon"></span><h1>Optimizing ONNX QuantizeLinear with ARM NEON for Efficient Edge Inference<a class="headerlink" href="#optimizing-onnx-quantizelinear-with-arm-neon-for-efficient-edge-inference" title="Link to this heading">¶</a></h1>
<p>Quantization is crucial for deploying deep learning models on resource-constrained edge devices. The ONNX
<code class="docutils literal notranslate"><span class="pre">QuantizeLinear</span></code> operator, while seemingly simple, can become a performance bottleneck if not optimized. This blog
post details an implementation of <code class="docutils literal notranslate"><span class="pre">QuantizeLinear</span></code> utilizing ARM NEON intrinsics, demonstrating significant speed
improvements crucial for efficient inference pipelines.</p>
<section id="onnx-quantizelinear-a-critical-pre-processing-step">
<h2>1. ONNX QuantizeLinear: A Critical Pre-Processing Step<a class="headerlink" href="#onnx-quantizelinear-a-critical-pre-processing-step" title="Link to this heading">¶</a></h2>
<p>In many deep learning inference pipelines targeting hardware accelerators like FPGAs, the <code class="docutils literal notranslate"><span class="pre">QuantizeLinear</span></code> operation
typically executes on the CPU. This initial quantization step converts floating-point activations into lower-precision
integer formats (e.g., <code class="docutils literal notranslate"><span class="pre">uint8</span></code> or <code class="docutils literal notranslate"><span class="pre">int8</span></code>). Subsequent computationally intensive operations, such as convolutions
(<code class="docutils literal notranslate"><span class="pre">Conv</span></code>) and matrix multiplications (<code class="docutils literal notranslate"><span class="pre">Gemm</span></code>), are then offloaded to the FPGA, which benefits from integer
arithmetic.</p>
<p>The latency introduced by <code class="docutils literal notranslate"><span class="pre">QuantizeLinear</span></code> directly impacts the overall inference speed. If this CPU-bound operation
is slow, it can negate the performance gains achieved by accelerating the core neural network operations on the FPGA.
Therefore, optimizing <code class="docutils literal notranslate"><span class="pre">QuantizeLinear</span></code> for speed is paramount.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">QuantizeLinear</span></code> operation performs the following transformation:</p>
<div class="math notranslate nohighlight">
\[Y = saturate(round(X / scale) + zero\_point)\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X\)</span> is the floating-point input tensor.</p></li>
<li><p><span class="math notranslate nohighlight">\(scale\)</span> is the quantization scale.</p></li>
<li><p><span class="math notranslate nohighlight">\(zero\_point\)</span> is the quantization zero point.</p></li>
<li><p><span class="math notranslate nohighlight">\(round\)</span> typically refers to rounding to the nearest even.</p></li>
<li><p><span class="math notranslate nohighlight">\(saturate\)</span> ensures the result fits within the target integer range (e.g., [0, 255] for <code class="docutils literal notranslate"><span class="pre">uint8</span></code>).</p></li>
</ul>
</section>
<section id="a-deep-dive-into-arm-neon-intrinsics">
<h2>2. A Deep Dive into ARM NEON Intrinsics<a class="headerlink" href="#a-deep-dive-into-arm-neon-intrinsics" title="Link to this heading">¶</a></h2>
<p>ARM NEON is a 128-bit SIMD (Single Instruction, Multiple Data) architecture extension for ARM Cortex-A series
processors. It provides a set of registers and instructions capable of performing parallel operations on multiple data
elements simultaneously. This parallelism is ideal for accelerating array-based computations common in deep learning,
including the element-wise operations involved in <code class="docutils literal notranslate"><span class="pre">QuantizeLinear</span></code>.</p>
<p>NEON intrinsics are C/C++ functions that map directly to NEON assembly instructions. They allow developers to leverage
the NEON hardware without writing assembly code, providing a balance between performance and development ease. Key NEON
intrinsics relevant to <code class="docutils literal notranslate"><span class="pre">QuantizeLinear</span></code> include:</p>
<ul class="simple">
<li><p><strong>Load/Store intrinsics (e.g., ``vld1q_f32``, ``vst1q_u8``):</strong> For efficient loading of floating-point data into NEON
registers and storing of quantized integer results.</p></li>
<li><p><strong>Vector arithmetic intrinsics (e.g., ``vmulq_f32``, ``vaddq_f32``, ``vrecpeq_f32`` for reciprocal estimation):</strong> For
performing the division, addition, and scaling operations in parallel on multiple floating-point values.</p></li>
<li><p><strong>Conversion intrinsics (e.g., ``vcvtq_s32_f32``, ``vqmovn_s32``):</strong> For converting floating-point numbers to integers
and performing saturating narrow operations to fit the target integer type.</p></li>
<li><p><strong>Rounding intrinsics (e.g., ``vrndnq_f32``):</strong> For implementing the rounding behavior specified by ONNX.</p></li>
<li><p><strong>Saturating intrinsics (e.g., ``vqadd_s8``, ``vqsub_s8``):</strong> For handling saturation when converting to lower
precision integer types. While direct saturating conversions are often handled by <code class="docutils literal notranslate"><span class="pre">vqmovn</span></code>, explicit saturation
intrinsics might be needed depending on the intermediate steps.</p></li>
</ul>
<p>By processing multiple data points within a single instruction cycle, NEON intrinsics significantly reduce the number of
CPU cycles required for the <code class="docutils literal notranslate"><span class="pre">QuantizeLinear</span></code> computation, leading to substantial speedups.</p>
</section>
<section id="speed-measurements-quantizelinear-with-neon-vs-scalar-cpu">
<h2>3. Speed Measurements: QuantizeLinear with NEON vs. Scalar CPU<a class="headerlink" href="#speed-measurements-quantizelinear-with-neon-vs-scalar-cpu" title="Link to this heading">¶</a></h2>
<p>To quantify the performance gains, benchmarks were conducted comparing a scalar C++ implementation of <code class="docutils literal notranslate"><span class="pre">QuantizeLinear</span></code>
against an ARM NEON-optimized version. The tests involved processing various input tensor sizes, simulating typical
activation dimensions in neural networks.</p>
<p>[Insert actual benchmark results here. This section should include:]</p>
<ul class="simple">
<li><p><strong>Methodology:</strong> Describe the test setup, including the ARM processor used (e.g., Cortex-A53, Cortex-A72), compiler
flags, and how timings were collected. Specify the input tensor dimensions and data types.</p></li>
<li><p><strong>Data Presentation:</strong> Present the speedup results clearly. This could be in the form of a table showing execution
times for scalar and NEON implementations, or a graph illustrating the speedup factor across different input sizes.</p></li>
<li><p><strong>Example Table:</strong></p></li>
</ul>
<ul class="simple">
<li><p><strong>Discussion of Results:</strong> Analyze the results. Highlight the magnitude of the speedup and discuss how NEON’s SIMD
capabilities contribute to this performance improvement. Emphasize the benefit for larger tensor sizes where the
overhead of scalar processing becomes more pronounced.</p></li>
</ul>
</section>
<section id="conclusion">
<h2>4. Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">¶</a></h2>
<p>Optimizing the <code class="docutils literal notranslate"><span class="pre">QuantizeLinear</span></code> operator with ARM NEON intrinsics delivers a critical performance boost for ONNX
models on ARM-based systems. By leveraging the parallel processing capabilities of NEON, the initial quantization step,
often a bottleneck, becomes significantly more efficient. This optimization ensures that the data is prepared rapidly
for subsequent accelerated operations on hardware like FPGAs, enabling a truly efficient end-to-end deep learning
inference pipeline on edge devices. The demonstrated speed improvements underscore the importance of low-level
optimization for maximizing the performance of AI applications in embedded environments.</p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Thoughts, et cetera</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Blog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sunday.html">Paper Sundays</a></li>
<li class="toctree-l1"><a class="reference internal" href="../links.html">Links</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="index.html">Blog</a><ul>
      <li>Previous: <a href="index.html" title="previous chapter">Blog</a></li>
      <li>Next: <a href="why_even_bother_with_fpgas.html" title="next chapter">Why Even Bother With FPGAs?</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2023, Shreeyash Pandey.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.1.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/blog/quantize_linear.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>