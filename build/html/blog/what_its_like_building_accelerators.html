<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>What it’s like to build an ML accelerator &#8212; Thoughts, et cetera  documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=d1102ebc" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=12dfc556" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=54f8742a" />
    <script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="what-it-s-like-to-build-an-ml-accelerator">
<h1>What it’s like to build an ML accelerator<a class="headerlink" href="#what-it-s-like-to-build-an-ml-accelerator" title="Link to this heading">¶</a></h1>
<p>Back in April 2023, at <a class="reference external" href="https://vicharak.in/">Vicharak</a>, I was tasked with
Finding a suitable application that can be implemented on Vaaman (an SBC with an
FPGA) as a part of a suite of applications.</p>
<p>FPGAs can, in theory, do almost anything (cue Turing completeness), but with a
catch: the programmer has to make it do whatever the task is. Such a task in the
CPU world is achieved through the installation of softwares authored by someone
else, or by writing one yourself (usually in a high-level programming language).
FPGAs, on the other hand,  are a blank slate that can be programmed to represent
any digital circuit. This is done through Hardware Description Languages, or
HDLs. Although a programming language, HDLs are not known to be very “user”
friendly and possess a very steep learning curve, making them a little tedious
to work with (especially to a non-technical person). But once programmed, FPGAs
are like dedicated integrated circuits, custom tailored to an application,
reconfigurable as need arises. So, The first goal of applications that I come up
with is the <strong>abstraction</strong> of this low-level interface. Something like CUDA (or
Open CL, for that matter) for our FPGA.</p>
<section id="the-open-cl-story">
<h2>The Open CL story<a class="headerlink" href="#the-open-cl-story" title="Link to this heading">¶</a></h2>
<p>Open CL is a specification that provides computing primitives and a
programming/memory model to go along with it. Its goal is to unify how different
processors (or heterogeneous processors, as they’re called) compute and interact
with each other. Being a specification, the implementation has to be provided by
the processor. In this case, the provider would be us.</p>
<p>The biggest challenge with designing an Open CL implementation is its
generality. It’s very useful to be able to run any algorithm with different and
complex control flow graphs on any processor of your choice. Supporting such a
requirement is a major challenge. One would be required to design a general
purpose ISA flexible enough to encompass any requirement by the specification
and fast enough to warrant the use of an FPGA (as opposed to some other
processor, like a GPU). High-level synthesis tools that convert high-level
languages (like C/Python) to HDL exist but are not known to squeeze in every
ounce of performance from the FPGA.</p>
<p>This led me to look for stricter subsets of algorithms that need acceleration.
Enter…</p>
</section>
<section id="deep-learning-neural-networks-convolutional-neural-networks">
<h2>Deep Learning (Neural Networks (Convolutional Neural Networks))<a class="headerlink" href="#deep-learning-neural-networks-convolutional-neural-networks" title="Link to this heading">¶</a></h2>
<p>Deep learning is a fast-growing area with active development and an interest in
the industry. Convolutional neural networks are a sub-class of deep learning
algorithms used for tasks such as classification, object detection,
segmentation, etc., of images or videos. A very peculiar feature of CNNs that
makes it the hero of our story is the presence of a rigid and fixed control-flow
graph of well-defined operations (or layers, for example, convolution, maxpool,
etc.), and a priori knowledge of the sizes and dimensions of data. This fits
precisely in the requirements that I had initially set.</p>
<p>So, how must one accelerate CNNs?</p>
<section id="gemm">
<h3>GEMM<a class="headerlink" href="#gemm" title="Link to this heading">¶</a></h3>
<p>“GEneral Matrix to Matrix Multiplication (or GEMM) is at the heart
of deep learning <span id="id1"><sup><a class="reference internal" href="#id23" title="Pete Warden. Why gemm is at the heart of deep learning. 2015. URL: https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/.">11</a></sup></span>”. Therefore, accelerating GEMMs would lead to
acceleration of deep learning algorithms in general. This is what I sought out
to do.</p>
<p>A GEMM is essentially a dot-product operation repeated K times, where K is the
number of rows and columns in the input matrices. Dot-product in itself is a
series of multiply-accumulate operations. Any accelerator, therefore, is mostly
a dot-product accelerator. <strong>A key takeaway here is that the workhorse of modern
deep learning is simple multiplication and addition over large dimensions.</strong></p>
<p>Consider a GEMM operation between two matrices <cite>A</cite> and <cite>B</cite> of dimensions ‘3x4’
and <cite>4x5</cite>. The product <cite>C</cite> is the result of row-column dot-product and would be
<cite>3x5</cite> in size.</p>
<p>To carry out GEMM on hardware, one would think of an arrangement of multipliers
in the form of a grid equal in size to the number of rows and columns in the
output matrix and stream rows of <cite>A</cite> from one end and columns of <cite>B</cite> from the
other. At the same time, throw some adders and a register along with the lonely
multiplier so that the results of previous operations can be accumulated and
fetched later. A demo can be found in this <a class="reference external" href="https://youtu.be/cmy7LBaWuZ8?si=AcwQVQi6pgiagcwf">YouTube video.</a>.</p>
<p>This is, in fact, what a systolic array is. A grid-like arrangement of PEs
(Processing Elements) capable of multiply-accumulate operations in a streaming
fashion. Systolic arrays have been known since the 1980s <span id="id2"><sup><a class="reference internal" href="#id24" title="Kung. Why systolic architectures? Computer, 15(1):37-46, 1982. doi:10.1109/MC.1982.1653825.">9</a></sup></span>, and
they can be (do) a lot more than multiply-accumulate. The only knobs to twist in
order for a systolic array to do whatever you want, are:</p>
<ol class="arabic simple">
<li><p>The arrangement of PEs (the dataflow).</p></li>
<li><p>What operation an individual PE can do.</p></li>
<li><p>How is data being fed into the array (this will be important later).</p></li>
</ol>
<p>For example, here’s a paper describing Systolic arrays are being used for GCD
computation of polynomials <span id="id3"><sup><a class="reference internal" href="#id25" title="Richard P. Brent and H. T. Kung. Systolic vlsi arrays for polynomial gcd computation. IEEE Transactions on Computers, C-33(8):731-736, 1984. doi:10.1109/TC.1984.5009358.">2</a></sup></span>.</p>
<p><strong>Matrix multiplication is typically an O(n³) problem. A systolic array can
do the same, given enough processors (PEs) and memory bandwidth, in O(n)
time.</strong></p>
</section>
<section id="cnns-and-convolutions">
<h3>CNNs and Convolutions<a class="headerlink" href="#cnns-and-convolutions" title="Link to this heading">¶</a></h3>
<p>Convolution is an operation where a kernel (a small matrix) is slid across an
input (a larger matrix) by a certain amount (called the ‘stride’), the
dot-product between the kernel and the parts of input that correspond to the
kernel forms the output. Elements in the output are as many as unique slides of
the kernel across the input. Here’s a really nice intro to understand what’s
really happening <span id="id4"><sup><a class="reference internal" href="#id26" title="Vincent Dumoulin and Francesco Visin. A guide to convolution arithmetic for deep learning. ArXiv e-prints, mar 2016. arXiv:1603.07285.">14</a></sup></span>.</p>
<p>Convolutions are a big deal in CNNs. They contribute 1/3 of that abbreviation.
after all! Them being dot-products is good news because we just designed a
dot-product accelerator a section ago. Strictly speaking, a convolution uses
same operations (dot product) as GEMMs, but is not GEMM. In order for a systolic
array to carry out convolutions, we have to slightly modify how we feed inputs.
Instead of feeding rows and columns, we statically load the elements of the
kernel in PEs (as a single kernel is being shared by all parts of the inputs),
transform the input by taking each slide and expanding it into a vector of KxK
elements (k is the size of the kernel) and feeding that. This transformation is
<em>im2col</em>.  <span id="id5"><sup><a class="reference internal" href="#id27" title="Leonardo Araujo dos Santos. Deep Learning - Making Faster. URL: https://leonardoaraujosantos.gitbook.io/artificial-inteligence/machine_learning/deep_learning/convolution_layer/making_faster.">3</a></sup></span>. A problem with this transformation is that the
input sizes expand as much as 9x the original. So, a 150KB input would become
1.3MB!. Expansion of input size renders storing the entire input on on-chip
memories futile (on-chip memories don’t tend to be very large), and induces
extra latency as we have to bring in a lot more data than we had to previously.</p>
<p>As the expansion pattern is pretty straight-forward, an algorithm can be
designed that takes the original inputs and expands them on-the-fly as they’re
required. Like a dynamic im2col as opposed to static. In fact, me and one of my
colleagues came up with not one but two algorithms to do this (sort of like
<a class="reference external" href="https://en.wikipedia.org/wiki/Leibniz%E2%80%93Newton_calculus_controversy">Newton and Liebniz</a>
 😛).</p>
</section>
<section id="dataflows-do-they-matter">
<h3>Dataflows: do they matter?<a class="headerlink" href="#dataflows-do-they-matter" title="Link to this heading">¶</a></h3>
<p>So far, two flavors of SAs have been introduced: one for GEMMs, the other for
Convolutions. These have names: output stationary and weight stationary
respectively. So-called because, in the former, outputs are accumulated and
stored in the PEs, and in the latter, weights (or kernels) are stored in the
PEs.  These are called <strong>dataflows</strong>.</p>
<p>What’s better? Output Stationary or Weight Stationary?</p>
<p>Both Good. Or, that is an incomplete question, as it does not account for the
memory bandwidth available. <em>DNN Dataflow choice is overrated</em> <span id="id6"><sup><a class="reference internal" href="#id28" title="Xuan Yang, Mingyu Gao, Qiaoyi Liu, Jeff Setter, Jing Pu, Ankita Nayak, Steven Bell, Kaidi Cao, Heonjae Ha, Priyanka Raina, Christos Kozyrakis, and Mark Horowitz. Interstellar: using halide’s scheduling language to analyze dnn accelerators. In Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS ’20. ACM, March 2020. URL: http://dx.doi.org/10.1145/3373376.3378514, doi:10.1145/3373376.3378514.">13</a></sup></span>
is an excellent paper on why choosing a dataflow alone (without respect to
memory) shouldn’t be pursued. <em>Memory matters because having endless compute
could mean endless performance, but only when endless data is available</em>.
Practically, only a finite amount of data is available, which bottlenecks
compute substantially.</p>
</section>
<section id="pipelines-are-awesome">
<h3>Pipelines are awesome<a class="headerlink" href="#pipelines-are-awesome" title="Link to this heading">¶</a></h3>
<p>A comp-arch literate reader might have already observed that systolic arrays
exhibit what’s called pipeline parallelism. For the uninformed, pipelines are a
division of labor where producers and consumers are chained sequentially.  Think
of an automobile manufacturing plant. In order to make a car, these are the
(grossly oversimplified) steps:</p>
<ol class="arabic simple">
<li><p>Assemble the chassis.</p></li>
<li><p>Assemble the engine.</p></li>
<li><p>Install interiors.</p></li>
<li><p>Install wheels.</p></li>
</ol>
<p>From raw materials, a chassis is assembled and sent to the engine assembly
plant. While the engine is being assembled for the first car, chassis assembly
plant can start assembling the second car. This process continues, and once a
finished car arrives out of the wheel assembly plant, all next iterations will
result in one finished car being produced. Replace cars with data, iterations
with cycles and plants with Processing Elements (PEs), and you’ve got a
computational pipeline. As opposed to CPU pipelines, where stages compute
different things, Systolic array pipelines are made of just one type of PE, viz.
Multiply-Accumulate. The number of PEs connected serially is the number of
stages in this pipeline. Pipelines can be imagined from the lowest to the
highest granularities.  With respect to convolutions, one can start with
pipelining computations of each channel, followed by kernels, followed by
pipelining network layers, executing the same image.  Zoom out even further, and
we can imagine not one but a batch of images being pipelined with multiple
copies of images.</p>
</section>
<section id="what-about-layers-that-are-not-convolutional">
<h3>What about layers that are not convolutional?<a class="headerlink" href="#what-about-layers-that-are-not-convolutional" title="Link to this heading">¶</a></h3>
<p>Non-Conv layers such as Maxpool, Fully Connected, and Relu do not occupy a
substantial percentage of compute, but they have to be accounted for. Take,
Maxpool, for example, it can be easily implemented as a separate block, but
Interesting architectures can come up if blocks such as Maxpool and Relu are
fused.  together to form a pipeline. Doing this almost completely hides the
latency one would’ve encountered had they iterated over the inputs
traditionally. The <a class="reference external" href="https://github.com/onnx/onnx/blob/main/docs/Operators.md">list of layers</a> to be accounted for
is large and diverse and poses interesting challenges. For example, element-wise
layers, such as add, mul, div, etc., and <em>memory-moving</em> layers, such as
Transpose and concat are not as compute-intensive as the actual computation part
is simple addition, multiplication, or raw data movement.  Most acceleration is
achieved when a large amount of data is processed in parallel. To process large
large data needs to be fetched from the memory.  This brings memory into the
picture. The latencies occurring in those layers are due to memory being a
bottleneck, and as a result, improving on compute will not take us much far.
Technically speaking, these layers have very low arithmetic intensities.
<span id="id7"><sup><a class="reference internal" href="#id29" title="Samuel Williams, Andrew Waterman, and David Patterson. Roofline: an insightful visual performance model for multicore architectures. Commun. ACM, 52(4):65–76, apr 2009. URL: https://doi.org/10.1145/1498765.1498785, doi:10.1145/1498765.1498785.">12</a></sup></span>. It is of paramount importance to understand the nature of
a layer (memory or compute) as they direct the design for architecture to
accelerate them. This distinction, however, is not binary. A layer needn’t be
exclusively compute-bound or memory-bound. Some can be both in different stages
of their acceleration. For example, convolution is compute bound when we are
limited by the number of MACs we can do in parallel, but as MACs increase, we
hit the <a class="reference external" href="https://en.wikipedia.org/wiki/Random-access_memory#Memory_wall">memory wall</a>. We can only
bring as much data as the memory bandwidth permits. Scaling compute beyond it
results in compute becoming idle.</p>
</section>
<section id="conclusion-so-far">
<h3>Conclusion so far<a class="headerlink" href="#conclusion-so-far" title="Link to this heading">¶</a></h3>
<p>Neural networks are made up of different layers, each with its own peculiarities
with respect to compute and memory accesses. Realization of the nature of each
layer shapes the architecture that would allow its acceleration. A concrete
execution pipeline must be designed for acceleration. Both compute and memory
are important.</p>
</section>
</section>
<section id="software-for-accelerators">
<h2>Software for accelerators<a class="headerlink" href="#software-for-accelerators" title="Link to this heading">¶</a></h2>
<p>Assume we’ve designed an accelerator and are ready to carry out inference on it.
What kinds of software do we need for this?</p>
<p>Right off the bat, since this accelerator exists in a heterogeneous environment,
data must be marshaled from its source to the accelerator. The source here is
the CPU running an Operating System, reading input from a video stream or a
static image. The CPU is also where the weights of the model are stored. Images,
weights and other metadata required by the accelerator should be sent through a
high-speed (obviously) link between both chips. The very first piece of software
would be a simple <em>compiler/runtime</em> that does this. It:</p>
<ol class="arabic simple">
<li><p>Reads relevant files or streams.</p></li>
<li><p>Transforms and appends additional metadata as required by the accelerator</p></li>
<li><p>Schedules it to be sent to the accelerator.</p></li>
</ol>
<p>Quite straight-forward.</p>
<section id="what-more-do-we-need-a-simulator-perhaps">
<h3>What more do we need? A simulator, perhaps?<a class="headerlink" href="#what-more-do-we-need-a-simulator-perhaps" title="Link to this heading">¶</a></h3>
<p>The design of an accelerator so far has required researching existing
algorithms  for a problem, mending it as per needs, optimizing for our cases, or
coming up with entirely new algorithms. All of these individual blocks, when
connected together form a rather complex system. The very first requirement from
this  system is <strong>correctness</strong>. One way to prove correctness is to write it
(albeit tediously) and see it for ourselves. The simulator is a tool to do that.
Written in a high-level language (compared to verilog) so that our algorithms
can be quickly verified. A simulator would ideally model the fine intricacies of
<strong>our</strong> architecture. Since we have a tool that emulates an architecture so
closely, we can also make it return latency information, given a model and an
architecture. I chose C++ for writing this simulator. Writing a simulator is a
non-trivial task; add to that the complexity inherited from the programming
language and we have a solid challenge at hand. In all honesty, programming this
has been the best part of this process for me.</p>
</section>
<section id="simulators-on-steroids">
<h3>Simulators on steroids<a class="headerlink" href="#simulators-on-steroids" title="Link to this heading">¶</a></h3>
<p>The following section is an adventure where I turn my imaginative knobs
one-at-a-time to envision possible directions that our simulator may take.</p>
<ol class="arabic simple">
<li><p>Suggestions of architectures</p></li>
</ol>
<p>Since the simulator is already pretty aware of what the FPGA is capable of given
an architecture. It is possible to turn the simulator into an architecture
predictor. The simulator knows how each layer in a given neural network should
be executed. Since there is more than one way to do it, a very naïve suggestion
algorithm could be to iterate over all the ways in which a layer can be
executed, measure the latencies, and provide other relevant metrics for each of
them and veto all but the more performant architecture (basically a search
algorithm). An obvious problem here would be the size of the search space, but
If certain constraints or heuristics were to be put in place, the search space
would become pretty tractable.</p>
<ol class="arabic simple" start="2">
<li><p>Neural nets for suggestion</p></li>
</ol>
<p>The first idea sounds a lot like an old-school AI algorithm. Possibility of an
old-school algorithm implies the possibility of a new-school algorithm, i.e., an
ML model. A trained ML model could be spun up to suggest a good architecture.
Sort of like <a class="reference external" href="https://en.wikipedia.org/wiki/Neural_architecture_search">Neural Architecture Search</a> but tailored to
generate dataflow architectures instead of ML architectures.</p>
<ol class="arabic simple" start="3">
<li><p>Generation of architectures</p></li>
</ol>
<p>Suggestions of architectures can be taken a step further into <em>Generation</em> of
architectures. This results in a sort of compiler for which the input is an ML
model, and the output is HDL, which most efficiently implements that model in
particular. This sounds like <a class="reference external" href="https://en.wikipedia.org/wiki/High-level_synthesis">High-Level Synthesis</a>, but this does not
involve any high-level general-purpose language. One of the most pressing issues
with HLS is the generality of what types of circuits can be expressed with a
general purpose language leads to the generation of HDL that may or may not be
exploiting every ounce of performance from the hardware. The idea here would be
somewhere in between HLS and writing HDL by hand: sort of like templated
highly-tuned kernels for common operations that one may encounter in a ML model.</p>
</section>
<section id="beyond-a-basic-accelerator">
<h3>Beyond a Basic Accelerator<a class="headerlink" href="#beyond-a-basic-accelerator" title="Link to this heading">¶</a></h3>
<p>Our accelerator so far exploits performance on an algorithmic level, but
large models with large FLOP requirements still require quite some time. If
we step back from accelerating a particular model (say, VGG16) to
accelerating the problem of ‘Image Classification’ or object detection, for
that matter, many new doors open up. The problems that large models solve can
also be solvedby smaller models (albeit with slightly lower accuracy). ML
models exhibit plenty of redundancy. So much so that one of the explanations
for their generalization is over-parameterization. As Simon Price says in
<a class="reference external" href="https://udlbook.github.io/udlbook/">Understanding Deep Learning [Chapter 20.5]</a>: <em>“There are almost no examples of
state-of-the-art performance on complex datasets where the model has
significantly fewer parameters than there were training data points. …
 it’s not clear if some fundamental property of smaller models prevents them
from performing as well or whether the training algorithms can’t find good
solutions for small models”</em>. Exploiting redundancies can lead to smaller
models that perform as well as their larger counterparts, with the added
benefit that they run faster. There are plenty of techniques to do this:
Quantization, value-based Optimizations, distillation, and pruning, to name a
few. A deeper discussion on these topics are for another article, but I’ll
leave you with a few resources (mostly on quantization, as I myself am not
very familiar with other techniques).</p>
<p><span id="id8"><sup><a class="reference internal" href="#id30" title="Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. 2017. arXiv:1712.05877.">5</a></sup></span> is a nice introduction to quantization as used by
Google in the TPU <span id="id9"><sup><a class="reference internal" href="#id31" title="Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. In-datacenter performance analysis of a tensor processing unit. 2017. arXiv:1704.04760.">6</a></sup></span>.  <span id="id10"><sup><a class="reference internal" href="#id32" title="Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. 2021. arXiv:2103.13630.">4</a></sup></span> is
a survey of techniques as of 2021.  <span id="id11"><sup><a class="reference internal" href="#id33" title="Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: a whitepaper. 2018. arXiv:1806.08342.">8</a></sup></span> is
quantization as used by PyTorch.  Intel’s Neural Distiller also has a small page
on Quantization Algorithms, <a class="reference external" href="https://intellabs.github.io/distiller/algo_quantization.html">here</a>.
<span id="id12"><sup><a class="reference internal" href="#id34" title="Patrick Judd, Alberto Delmas, Sayeh Sharify, and Andreas Moshovos. Cnvlutin2: ineffectual-activation-and-weight-free deep neural network computing. 2017. arXiv:1705.00125.">7</a></sup></span>, <span id="id13"><sup><a class="reference internal" href="#id35" title="J. Albericio, P. Judd, A. Delmás, S. Sharify, and A. Moshovos. Bit-pragmatic deep neural network computing. 2016. arXiv:1610.06920.">1</a></sup></span>, and
<span id="id14"><sup><a class="reference internal" href="#id36" title="Andreas Moshovos, Jorge Albericio, Patrick Judd, Alberto Delmás Lascorz, Sayeh Sharify, Tayler Hetherington, Tor Aamodt, and Natalie Enright Jerger. Value-based deep-learning acceleration. IEEE Micro, 38(1):41-55, 2018. doi:10.1109/MM.2018.112130309.">10</a></sup></span> are good introduction to value-based
optimizations.</p>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">¶</a></h2>
<div class="docutils container" id="id15">
<div role="list" class="citation-list">
<div class="citation" id="id35" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">1</a><span class="fn-bracket">]</span></span>
<p>J. Albericio, P. Judd, A. Delmás, S. Sharify, and A. Moshovos. Bit-pragmatic deep neural network computing. 2016. <a class="reference external" href="https://arxiv.org/abs/1610.06920">arXiv:1610.06920</a>.</p>
</div>
<div class="citation" id="id25" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">2</a><span class="fn-bracket">]</span></span>
<p>Richard P. Brent and H. T. Kung. Systolic vlsi arrays for polynomial gcd computation. <em>IEEE Transactions on Computers</em>, C-33(8):731–736, 1984. <a class="reference external" href="https://doi.org/10.1109/TC.1984.5009358">doi:10.1109/TC.1984.5009358</a>.</p>
</div>
<div class="citation" id="id27" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">3</a><span class="fn-bracket">]</span></span>
<p>Leonardo Araujo dos Santos. Deep Learning - Making Faster. URL: <a class="reference external" href="https://leonardoaraujosantos.gitbook.io/artificial-inteligence/machine_learning/deep_learning/convolution_layer/making_faster">https://leonardoaraujosantos.gitbook.io/artificial-inteligence/machine_learning/deep_learning/convolution_layer/making_faster</a>.</p>
</div>
<div class="citation" id="id32" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">4</a><span class="fn-bracket">]</span></span>
<p>Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. 2021. <a class="reference external" href="https://arxiv.org/abs/2103.13630">arXiv:2103.13630</a>.</p>
</div>
<div class="citation" id="id30" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">5</a><span class="fn-bracket">]</span></span>
<p>Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. 2017. <a class="reference external" href="https://arxiv.org/abs/1712.05877">arXiv:1712.05877</a>.</p>
</div>
<div class="citation" id="id31" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">6</a><span class="fn-bracket">]</span></span>
<p>Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. In-datacenter performance analysis of a tensor processing unit. 2017. <a class="reference external" href="https://arxiv.org/abs/1704.04760">arXiv:1704.04760</a>.</p>
</div>
<div class="citation" id="id34" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">7</a><span class="fn-bracket">]</span></span>
<p>Patrick Judd, Alberto Delmas, Sayeh Sharify, and Andreas Moshovos. Cnvlutin2: ineffectual-activation-and-weight-free deep neural network computing. 2017. <a class="reference external" href="https://arxiv.org/abs/1705.00125">arXiv:1705.00125</a>.</p>
</div>
<div class="citation" id="id33" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">8</a><span class="fn-bracket">]</span></span>
<p>Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: a whitepaper. 2018. <a class="reference external" href="https://arxiv.org/abs/1806.08342">arXiv:1806.08342</a>.</p>
</div>
<div class="citation" id="id24" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">9</a><span class="fn-bracket">]</span></span>
<p>Kung. Why systolic architectures? <em>Computer</em>, 15(1):37–46, 1982. <a class="reference external" href="https://doi.org/10.1109/MC.1982.1653825">doi:10.1109/MC.1982.1653825</a>.</p>
</div>
<div class="citation" id="id36" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">10</a><span class="fn-bracket">]</span></span>
<p>Andreas Moshovos, Jorge Albericio, Patrick Judd, Alberto Delmás Lascorz, Sayeh Sharify, Tayler Hetherington, Tor Aamodt, and Natalie Enright Jerger. Value-based deep-learning acceleration. <em>IEEE Micro</em>, 38(1):41–55, 2018. <a class="reference external" href="https://doi.org/10.1109/MM.2018.112130309">doi:10.1109/MM.2018.112130309</a>.</p>
</div>
<div class="citation" id="id23" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">11</a><span class="fn-bracket">]</span></span>
<p>Pete Warden. Why gemm is at the heart of deep learning. 2015. URL: <a class="reference external" href="https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/">https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/</a>.</p>
</div>
<div class="citation" id="id29" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">12</a><span class="fn-bracket">]</span></span>
<p>Samuel Williams, Andrew Waterman, and David Patterson. Roofline: an insightful visual performance model for multicore architectures. <em>Commun. ACM</em>, 52(4):65–76, apr 2009. URL: <a class="reference external" href="https://doi.org/10.1145/1498765.1498785">https://doi.org/10.1145/1498765.1498785</a>, <a class="reference external" href="https://doi.org/10.1145/1498765.1498785">doi:10.1145/1498765.1498785</a>.</p>
</div>
<div class="citation" id="id28" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">13</a><span class="fn-bracket">]</span></span>
<p>Xuan Yang, Mingyu Gao, Qiaoyi Liu, Jeff Setter, Jing Pu, Ankita Nayak, Steven Bell, Kaidi Cao, Heonjae Ha, Priyanka Raina, Christos Kozyrakis, and Mark Horowitz. Interstellar: using halide’s scheduling language to analyze dnn accelerators. In <em>Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</em>, ASPLOS ’20. ACM, March 2020. URL: <a class="reference external" href="http://dx.doi.org/10.1145/3373376.3378514">http://dx.doi.org/10.1145/3373376.3378514</a>, <a class="reference external" href="https://doi.org/10.1145/3373376.3378514">doi:10.1145/3373376.3378514</a>.</p>
</div>
<div class="citation" id="id26" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">14</a><span class="fn-bracket">]</span></span>
<p>Vincent Dumoulin and Francesco Visin. A guide to convolution arithmetic for deep learning. <em>ArXiv e-prints</em>, mar 2016. <a class="reference external" href="https://arxiv.org/abs/1603.07285">arXiv:1603.07285</a>.</p>
</div>
</div>
</div>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Thoughts, et cetera</a></h1>








<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">Blog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sunday.html">Paper Sundays</a></li>
<li class="toctree-l1"><a class="reference internal" href="../links.html">Links</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2023, Shreeyash Pandey.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 7.3.7</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
      |
      <a href="../_sources/blog/what_its_like_building_accelerators.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>