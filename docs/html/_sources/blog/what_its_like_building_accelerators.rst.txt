What it's like to build an ML accelerator
#########################################

Back in April 2023, at `Vicharak <https://vicharak.in/>`_, I was tasked with
Finding a suitable application that can be implemented on Vaaman (an SBC with an
FPGA) as a part of a suite of applications.

FPGAs can, in theory, do almost anything (cue Turing completeness), but with a
catch: the programmer has to make it do whatever the task is. Such a task in the
CPU world is achieved through the installation of softwares authored by someone
else, or by writing one yourself (usually in a high-level programming language).
FPGAs, on the other hand,Â  are a blank slate that can be programmed to represent
any digital circuit. This is done through Hardware Description Languages, or
HDLs. Although a programming language, HDLs are not known to be very "user"
friendly and possess a very steep learning curve, making them a little tedious
to work with (especially to a non-technical person). But once programmed, FPGAs
are like dedicated integrated circuits, custom tailored to an application,
reconfigurable as need arises. So, The first goal of applications that I come up
with is the **abstraction** of this low-level interface. Something like CUDA (or
Open CL, for that matter) for our FPGA.Â 

The Open CL story 
-----------------

Open CL is a specification that provides computing primitives and a
programming/memory model to go along with it. Its goal is to unify how different
processors (or heterogeneous processors, as they're called) compute and interact
with each other. Being a specification, the implementation has to be provided by
the processor. In this case, the provider would be us.

The biggest challenge with designing an Open CL implementation is its
generality. It's very useful to be able to run any algorithm with different and
complex control flow graphs on any processor of your choice. Supporting such a
requirement is a major challenge. One would be required to design a general
purpose ISA flexible enough to encompass any requirement by the specification
and fast enough to warrant the use of an FPGA (as opposed to some other
processor, like a GPU). High-level synthesis tools that convert high-level
languages (like C/Python) to HDL exist but are not known to squeeze in every
ounce of performance from the FPGA.

This led me to look for stricter subsets of algorithms that need acceleration.
Enter...

Deep Learning (Neural Networks (Convolutional Neural Networks))
------------------------------------------------------------------

Deep learning is a fast-growing area with active development and an interest in
the industry. Convolutional neural networks are a sub-class of deep learning
algorithms used for tasks such as classification, object detection,
segmentation, etc., of images or videos. A very peculiar feature of CNNs that
makes it the hero of our story is the presence of a rigid and fixed control-flow
graph of well-defined operations (or layers, for example, convolution, maxpool,
etc.), and a priori knowledge of the sizes and dimensions of data. This fits
precisely in the requirements that I had initially set.

So, how must one accelerate CNNs?Â 

GEMM 
====

"GEneral Matrix to Matrix Multiplication (or GEMM) is at the heart
of deep learning :cite:`pete2015`". Therefore, accelerating GEMMs would lead to
acceleration of deep learning algorithms in general. This is what I sought out
to do.

A GEMM is essentially a dot-product operation repeated K times, where K is the
number of rows and columns in the input matrices. Dot-product in itself is a
series of multiply-accumulate operations. Any accelerator, therefore, is mostly
a dot-product accelerator. **A key takeaway here is that the workhorse of modern
deep learning is simple multiplication and addition over large dimensions.**

Consider a GEMM operation between two matrices `A` and `B` of dimensions '3x4'Â 
and `4x5`. The product `C` is the result of row-column dot-product and would be
`3x5` in size.Â 

To carry out GEMM on hardware, one would think of an arrangement of multipliers
in the form of a grid equal in size to the number of rows and columns in the
output matrix and stream rows of `A` from one end and columns of `B` from the
other. At the same time, throw some adders and a register along with the lonely
multiplier so that the results of previous operations can be accumulated and
fetched later. A demo can be found in this `YouTube video.
<https://youtu.be/cmy7LBaWuZ8?si=AcwQVQi6pgiagcwf>`_.

This is, in fact, what a systolic array is. A grid-like arrangement of PEs
(Processing Elements) capable of multiply-accumulate operations in a streaming
fashion. Systolic arrays have been known since the 1980s :cite:`kung1982`, and
they can be (do) a lot more than multiply-accumulate. The only knobs to twist in
order for a systolic array to do whatever you want, are:

1. The arrangement of PEs (the dataflow).

2. What operation an individual PE can do.

3. How is data being fed into the array (this will be important later).

For example, here's a paper describing Systolic arrays are being used for GCD
computation of polynomials :cite:`brent1984`.

**Matrix multiplication is typically an O(nÂ³) problem. A systolic array can
do the same, given enough processors (PEs) and memory bandwidth, in O(n)
time.**

CNNs and Convolutions 
=====================

Convolution is an operation where a kernel (a small matrix) is slid across an
input (a larger matrix) by a certain amount (called the 'stride'), the
dot-product between the kernel and the parts of input that correspond to the
kernel forms the output. Elements in the output are as many as unique slides of
the kernel across the input. Here's a really nice intro to understand what's
really happening :cite:`dumoulin2016`.Â 

Convolutions are a big deal in CNNs. They contribute 1/3 of that abbreviation.
after all! Them being dot-products is good news because we just designed a
dot-product accelerator a section ago. Strictly speaking, a convolution uses
same operations (dot product) as GEMMs, but is not GEMM. In order for a systolic
array to carry out convolutions, we have to slightly modify how we feed inputs.
Instead of feeding rows and columns, we statically load the elements of the
kernel in PEs (as a single kernel is being shared by all parts of the inputs),
transform the input by taking each slide and expanding it into a vector of KxK
elements (k is the size of the kernel) and feeding that. This transformation is
*im2col*.  :cite:`santosim2col`. A problem with this transformation is that the
input sizes expand as much as 9x the original. So, a 150KB input would becomeÂ 
1.3MB!. Expansion of input size renders storing the entire input on on-chip
memories futile (on-chip memories don't tend to be very large), and induces
extra latency as we have to bring in a lot more data than we had to previously.Â 

As the expansion pattern is pretty straight-forward, an algorithm can be
designed that takes the original inputs and expands them on-the-fly as they're
required. Like a dynamic im2col as opposed to static. In fact, me and one of my
colleagues came up with not one but two algorithms to do this (sort of like
`Newton and LiebnizÂ 
<https://en.wikipedia.org/wiki/Leibniz%E2%80%93Newton_calculus_controversy>`_
Â ðŸ˜›).

Dataflows: do they matter?  
=============================

So far, two flavors of SAs have been introduced: one for GEMMs, the other for
Convolutions. These have names: output stationary and weight stationary
respectively. So-called because, in the former, outputs are accumulated and
stored in the PEs, and in the latter, weights (or kernels) are stored in the
PEs.  These are called **dataflows**.

What's better? Output Stationary or Weight Stationary?Â 

Both Good. Or, that is an incomplete question, as it does not account for the
memory bandwidth available. *DNN Dataflow choice is overrated* :cite:`yang2018`
is an excellent paper on why choosing a dataflow alone (without respect to
memory) shouldn't be pursued. *Memory matters because having endless compute
could mean endless performance, but only when endless data is available*.
Practically, only a finite amount of data is available, which bottlenecksÂ 
compute substantially.

Pipelines are awesome 
=====================

A comp-arch literate reader might have already observed that systolic arrays
exhibit what's called pipeline parallelism. For the uninformed, pipelines are a
division of labor where producers and consumers are chained sequentially.  Think
of an automobile manufacturing plant. In order to make a car, these are the
(grossly oversimplified) steps:

1. Assemble the chassis.  

2. Assemble the engine.  

3. Install interiors.  

4.  Install wheels.

From raw materials, a chassis is assembled and sent to the engine assembly
plant. While the engine is being assembled for the first car, chassis assembly
plant can start assembling the second car. This process continues, and once a
finished car arrives out of the wheel assembly plant, all next iterations will
result in one finished car being produced. Replace cars with data, iterations
with cycles and plants with Processing Elements (PEs), and you've got a
computational pipeline. As opposed to CPU pipelines, where stages compute
different things, Systolic array pipelines are made of just one type of PE, viz.
Multiply-Accumulate. The number of PEs connected serially is the number of
stages in this pipeline. Pipelines can be imagined from the lowest to the
highest granularities.  With respect to convolutions, one can start with
pipelining computations of each channel, followed by kernels, followed by
pipelining network layers, executing the same image.  Zoom out even further, and
we can imagine not one but a batch of images being pipelined with multiple
copies of images.

What about layers that are not convolutional?
=============================================

Non-Conv layers such as Maxpool, Fully Connected, and Relu do not occupy a
substantial percentage of compute, but they have to be accounted for. Take,
Maxpool, for example, it can be easily implemented as a separate block, but
Interesting architectures can come up if blocks such as Maxpool and Relu are
fused.  together to form a pipeline. Doing this almost completely hides the
latency one would've encountered had they iterated over the inputs
traditionally. The `list of layers
<https://github.com/onnx/onnx/blob/main/docs/Operators.md>`_ to be accounted for
is large and diverse and poses interesting challenges. For example, element-wise
layers, such as add, mul, div, etc., and *memory-moving* layers, such as
Transpose and concat are not as compute-intensive as the actual computation part
is simple addition, multiplication, or raw data movement.  Most acceleration is
achieved when a large amount of data is processed in parallel. To process large
large data needs to be fetched from the memory.  This brings memory into the
picture. The latencies occurring in those layers are due to memory being a
bottleneck, and as a result, improving on compute will not take us much far.
Technically speaking, these layers have very low arithmetic intensities.
:cite:`williams2009`. It is of paramount importance to understand the nature of
a layer (memory or compute) as they direct the design for architecture to
accelerate them. This distinction, however, is not binary. A layer needn't be
exclusively compute-bound or memory-bound. Some can be both in different stages
of their acceleration. For example, convolution is compute bound when we are
limited by the number of MACs we can do in parallel, but as MACs increase, we
hit the `memory wall
<https://en.wikipedia.org/wiki/Random-access_memory#Memory_wall>`_. We can only
bring as much data as the memory bandwidth permits. Scaling compute beyond it
results in compute becoming idle.


Conclusion so far 
=================

Neural networks are made up of different layers, each with its own peculiarities
with respect to compute and memory accesses. Realization of the nature of each
layer shapes the architecture that would allow its acceleration. A concrete
execution pipeline must be designed for acceleration. Both compute and memory
are important.

Software for accelerators 
--------------------------

Assume we've designed an accelerator and are ready to carry out inference on it.
What kinds of software do we need for this?Â 

Right off the bat, since this accelerator exists in a heterogeneous environment,
data must be marshaled from its source to the accelerator. The source here is
the CPU running an Operating System, reading input from a video stream or a
static image. The CPU is also where the weights of the model are stored. Images,
weights and other metadata required by the accelerator should be sent through a
high-speed (obviously) link between both chips. The very first piece of software
would be a simple *compiler/runtime* that does this. It:

1. Reads relevant files or streams.  

2. Transforms and appends additional metadata as required by the accelerator 

3. Schedules it to be sent to the accelerator.

Quite straight-forward.

What more do we need? A simulator, perhaps?
===========================================

The design of an accelerator so far has required researching existing
algorithmsÂ  for a problem, mending it as per needs, optimizing for our cases, or
coming up with entirely new algorithms. All of these individual blocks, when
connected together form a rather complex system. The very first requirement from
thisÂ  system is **correctness**. One way to prove correctness is to write it
(albeit tediously) and see it for ourselves. The simulator is a tool to do that.
Written in a high-level language (compared to verilog) so that our algorithms
can be quickly verified. A simulator would ideally model the fine intricacies of
**our** architecture. Since we have a tool that emulates an architecture so
closely, we can also make it return latency information, given a model and an
architecture. I chose C++ for writing this simulator. Writing a simulator is a
non-trivial task; add to that the complexity inherited from the programming
language and we have a solid challenge at hand. In all honesty, programming this
has been the best part of this process for me.Â 

Simulators on steroids 
======================

The following section is an adventure where I turn my imaginative knobs
one-at-a-time to envision possible directions that our simulator may take.

1. Suggestions of architectures

Since the simulator is already pretty aware of what the FPGA is capable of given
an architecture. It is possible to turn the simulator into an architecture
predictor. The simulator knows how each layer in a given neural network should
be executed. Since there is more than one way to do it, a very naÃ¯ve suggestion
algorithm could be to iterate over all the ways in which a layer can be
executed, measure the latencies, and provide other relevant metrics for each of
them and veto all but the more performant architecture (basically a search
algorithm). An obvious problem here would be the size of the search space, but
If certain constraints or heuristics were to be put in place, the search space
would become pretty tractable.

2. Neural nets for suggestion

The first idea sounds a lot like an old-school AI algorithm. Possibility of an
old-school algorithm implies the possibility of a new-school algorithm, i.e., an
ML model. A trained ML model could be spun up to suggest a good architecture.
Sort of like `Neural Architecture Search
<https://en.wikipedia.org/wiki/Neural_architecture_search>`_ but tailored to
generate dataflow architectures instead of ML architectures.

3. Generation of architectures

Suggestions of architectures can be taken a step further into *Generation* of
architectures. This results in a sort of compiler for which the input is an ML
model, and the output is HDL, which most efficiently implements that model in
particular. This sounds like `High-Level Synthesis
<https://en.wikipedia.org/wiki/High-level_synthesis>`_, but this does not
involve any high-level general-purpose language. One of the most pressing issues
with HLS is the generality of what types of circuits can be expressed with a
general purpose language leads to the generation of HDL that may or may not be
exploiting every ounce of performance from the hardware. The idea here would be
somewhere in between HLS and writing HDL by hand: sort of like templated
highly-tuned kernels for common operations that one may encounter in a ML model.

Beyond a Basic Accelerator 
==========================

Our accelerator so far exploits performance on an algorithmic level, but
large models with large FLOP requirements still require quite some time. If
we step back from accelerating a particular model (say, VGG16) to
accelerating the problem of 'Image Classification' or object detection, for
that matter, many new doors open up. The problems that large models solve can
also be solvedby smaller models (albeit with slightly lower accuracy). ML
models exhibit plenty of redundancy. So much so that one of the explanations
for their generalization is over-parameterization. As Simon Price says in
`Understanding Deep Learning [Chapter 20.5]
<https://udlbook.github.io/udlbook/>`_: *"There are almost no examples of
state-of-the-art performance on complex datasets where the model has
significantly fewer parameters than there were training data points. ...
Â itâ€™s not clear if some fundamental property of smaller models prevents them
from performing as well or whether the training algorithms canâ€™t find good
solutions for small models"*. Exploiting redundancies can lead to smaller
models that perform as well as their larger counterparts, with the added
benefit that they run faster. There are plenty of techniques to do this:
Quantization, value-based Optimizations, distillation, and pruning, to name a
few. A deeper discussion on these topics are for another article, but I'll
leave you with a few resources (mostly on quantization, as I myself am not
very familiar with other techniques).

:cite:`jacob2017quantization` is a nice introduction to quantization as used by
Google in the TPU :cite:`jouppi2017indatacenter`.  :cite:`gholami2021survey` is
a survey of techniques as of 2021.  :cite:`krishnamoorthi2018quantizing` is
quantization as used by PyTorch.  Intel's Neural Distiller also has a small page
on Quantization Algorithms, `here
<https://intellabs.github.io/distiller/algo_quantization.html>`_.Â 
:cite:`judd2017cnvlutin2`, :cite:`albericio2016bitpragmatic`, and
:cite:`valuebased2018moshovos` are good introduction to value-based
optimizations.

References 
----------

.. bibliography::
