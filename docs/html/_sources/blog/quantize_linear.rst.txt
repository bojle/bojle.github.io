.. _onnx_quantizelinear_neon:

Optimizing ONNX QuantizeLinear with ARM NEON for Efficient Edge Inference
========================================================================

Quantization is crucial for deploying deep learning models on resource-constrained edge devices. The ONNX
``QuantizeLinear`` operator, while seemingly simple, can become a performance bottleneck if not optimized. This blog
post details an implementation of ``QuantizeLinear`` utilizing ARM NEON intrinsics, demonstrating significant speed
improvements crucial for efficient inference pipelines.

1. ONNX QuantizeLinear: A Critical Pre-Processing Step
-----------------------------------------------------

In many deep learning inference pipelines targeting hardware accelerators like FPGAs, the ``QuantizeLinear`` operation
typically executes on the CPU. This initial quantization step converts floating-point activations into lower-precision
integer formats (e.g., ``uint8`` or ``int8``). Subsequent computationally intensive operations, such as convolutions
(``Conv``) and matrix multiplications (``Gemm``), are then offloaded to the FPGA, which benefits from integer
arithmetic.

The latency introduced by ``QuantizeLinear`` directly impacts the overall inference speed. If this CPU-bound operation
is slow, it can negate the performance gains achieved by accelerating the core neural network operations on the FPGA.
Therefore, optimizing ``QuantizeLinear`` for speed is paramount.

The ``QuantizeLinear`` operation performs the following transformation:

.. math:: Y = saturate(round(X / scale) + zero\_point)

Where:

* :math:`X` is the floating-point input tensor.
* :math:`scale` is the quantization scale.
* :math:`zero\_point` is the quantization zero point.
* :math:`round` typically refers to rounding to the nearest even.
* :math:`saturate` ensures the result fits within the target integer range (e.g., [0, 255] for ``uint8``).

2. A Deep Dive into ARM NEON Intrinsics
---------------------------------------

ARM NEON is a 128-bit SIMD (Single Instruction, Multiple Data) architecture extension for ARM Cortex-A series
processors. It provides a set of registers and instructions capable of performing parallel operations on multiple data
elements simultaneously. This parallelism is ideal for accelerating array-based computations common in deep learning,
including the element-wise operations involved in ``QuantizeLinear``.

NEON intrinsics are C/C++ functions that map directly to NEON assembly instructions. They allow developers to leverage
the NEON hardware without writing assembly code, providing a balance between performance and development ease. Key NEON
intrinsics relevant to ``QuantizeLinear`` include:

* **Load/Store intrinsics (e.g., ``vld1q_f32``, ``vst1q_u8``):** For efficient loading of floating-point data into NEON
  registers and storing of quantized integer results.

* **Vector arithmetic intrinsics (e.g., ``vmulq_f32``, ``vaddq_f32``, ``vrecpeq_f32`` for reciprocal estimation):** For
  performing the division, addition, and scaling operations in parallel on multiple floating-point values.

* **Conversion intrinsics (e.g., ``vcvtq_s32_f32``, ``vqmovn_s32``):** For converting floating-point numbers to integers
  and performing saturating narrow operations to fit the target integer type.

* **Rounding intrinsics (e.g., ``vrndnq_f32``):** For implementing the rounding behavior specified by ONNX.

* **Saturating intrinsics (e.g., ``vqadd_s8``, ``vqsub_s8``):** For handling saturation when converting to lower
  precision integer types. While direct saturating conversions are often handled by ``vqmovn``, explicit saturation
  intrinsics might be needed depending on the intermediate steps.

By processing multiple data points within a single instruction cycle, NEON intrinsics significantly reduce the number of
CPU cycles required for the ``QuantizeLinear`` computation, leading to substantial speedups.

3. Speed Measurements: QuantizeLinear with NEON vs. Scalar CPU
--------------------------------------------------------------

To quantify the performance gains, benchmarks were conducted comparing a scalar C++ implementation of ``QuantizeLinear``
against an ARM NEON-optimized version. The tests involved processing various input tensor sizes, simulating typical
activation dimensions in neural networks.

[Insert actual benchmark results here. This section should include:]

* **Methodology:** Describe the test setup, including the ARM processor used (e.g., Cortex-A53, Cortex-A72), compiler
  flags, and how timings were collected. Specify the input tensor dimensions and data types.
* **Data Presentation:** Present the speedup results clearly. This could be in the form of a table showing execution
  times for scalar and NEON implementations, or a graph illustrating the speedup factor across different input sizes.

* **Example Table:**

+---------------------+--------------------+--------------+----------------+
| Input Size (Elements)| Scalar CPU Time (ms)| NEON Time (ms)| Speedup Factor |
+=====================+====================+==============+================+
| 1024                | X.XX               | Y.YY         | Z.Z            |
+---------------------+--------------------+--------------+----------------+
| 4096                | A.AA               | B.BB         | C.C            |
+---------------------+--------------------+--------------+----------------+
| 16384               | D.DD               | E.EE         | F.F            |
+---------------------+--------------------+--------------+----------------+

* **Discussion of Results:** Analyze the results. Highlight the magnitude of the speedup and discuss how NEON's SIMD
  capabilities contribute to this performance improvement. Emphasize the benefit for larger tensor sizes where the
  overhead of scalar processing becomes more pronounced.

4. Conclusion
-------------

Optimizing the ``QuantizeLinear`` operator with ARM NEON intrinsics delivers a critical performance boost for ONNX
models on ARM-based systems. By leveraging the parallel processing capabilities of NEON, the initial quantization step,
often a bottleneck, becomes significantly more efficient. This optimization ensures that the data is prepared rapidly
for subsequent accelerated operations on hardware like FPGAs, enabling a truly efficient end-to-end deep learning
inference pipeline on edge devices. The demonstrated speed improvements underscore the importance of low-level
optimization for maximizing the performance of AI applications in embedded environments.
